{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cbb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"subhaditya/fer2013plus\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def load_data(\n",
    "    path_prefix,\n",
    "    dataset_name,\n",
    "    splits=['train', 'test'],\n",
    "):\n",
    "    X, y = {}, {}\n",
    "\n",
    "    IMG_SIZE = 224 if 'RAFDB' in dataset_name else 120\n",
    "    splits = ['train', 'test'] if 'RAFDB' in dataset_name else splits\n",
    "    classNames = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise'] if 'RAFDB' in dataset_name else ['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "    for split in splits:\n",
    "        PATH = os.path.join(path_prefix, dataset_name, split)\n",
    "        X[split], y[split] = [], []\n",
    "        for classes in os.listdir(PATH):\n",
    "            class_path = os.path.join(PATH, classes)\n",
    "            class_numeric = classNames.index(classes)\n",
    "            for sample in os.listdir(class_path):\n",
    "                sample_path = os.path.join(class_path, sample)\n",
    "                image = cv2.imread(sample_path, cv2.IMREAD_COLOR)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                X[split].append(image)\n",
    "                y[split].append(class_numeric)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    for split in splits:\n",
    "        X[split] = np.array(X[split])\n",
    "        y[split] = np.array(y[split])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data('../kaggle/input/fer2013plus/fer2013plus', dataset_name='fer2013')\n",
    "\n",
    "with h5py.File('ferp.h5', 'w') as dataset:\n",
    "    for split in X.keys():\n",
    "        dataset.create_dataset(f'X_{split}', data=X[split])\n",
    "        dataset.create_dataset(f'y_{split}', data=y[split])\n",
    "\n",
    "del X, y\n",
    "NUM_CLASSES = 8\n",
    "IMG_SHAPE = (120, 120, 3)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "TRAIN_EPOCH = 100\n",
    "TRAIN_LR = 1e-3\n",
    "TRAIN_ES_PATIENCE = 5\n",
    "TRAIN_LR_PATIENCE = 3\n",
    "TRAIN_MIN_LR = 1e-6\n",
    "TRAIN_DROPOUT = 0.1\n",
    "\n",
    "FT_EPOCH = 500\n",
    "FT_LR = 1e-5\n",
    "FT_LR_DECAY_STEP = 80.0\n",
    "FT_LR_DECAY_RATE = 1\n",
    "FT_ES_PATIENCE = 20\n",
    "FT_DROPOUT = 0.2\n",
    "\n",
    "ES_LR_MIN_DELTA = 0.003\n",
    "\n",
    "# Load your data here, PAtt-Lite was trained with h5py for shorter loading time\n",
    "with h5py.File('ferp.h5', 'r') as dataset:\n",
    "    X_train = np.array(dataset['X_train'])\n",
    "    y_train = np.array(dataset['y_train'])\n",
    "    X_test = np.array(dataset['X_test'])\n",
    "    y_test = np.array(dataset['y_test'])\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Shape of train_sample: {}\".format(X_train.shape))\n",
    "print(\"Shape of train_label: {}\".format(y_train.shape))\n",
    "print(\"Shape of test_sample: {}\".format(X_test.shape))\n",
    "print(\"Shape of test_label: {}\".format(y_test.shape))\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Model Building\n",
    "input_layer = tf.keras.Input(shape=IMG_SHAPE, name='universal_input')\n",
    "sample_resizing = tf.keras.layers.Resizing(224, 224, name=\"resize\")\n",
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(mode='horizontal'),\n",
    "                                        tf.keras.layers.RandomContrast(factor=0.3)], name=\"augmentation\")\n",
    "preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
    "\n",
    "backbone = tf.keras.applications.mobilenet.MobileNet(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "backbone.trainable = False\n",
    "base_model = tf.keras.Model(backbone.input, backbone.layers[-29].output, name='base_model')\n",
    "\n",
    "# Remove the self_attention layer since it's causing dimension mismatch\n",
    "# self_attention = tf.keras.layers.Attention(use_scale=True, name='attention')\n",
    "\n",
    "patch_extraction = tf.keras.Sequential([\n",
    "    tf.keras.layers.SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'),\n",
    "    tf.keras.layers.SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n",
    "], name='patch_extraction')\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D(name='gap')\n",
    "pre_classification = tf.keras.Sequential([tf.keras.layers.Dense(32, activation='relu'),\n",
    "                                          tf.keras.layers.BatchNormalization()], name='pre_classification')\n",
    "prediction_layer = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name='classification_head')\n",
    "\n",
    "# First training phase\n",
    "inputs = input_layer\n",
    "x = sample_resizing(inputs)\n",
    "x = data_augmentation(x)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = patch_extraction(x)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(TRAIN_DROPOUT)(x)\n",
    "x = pre_classification(x)\n",
    "# Remove the self_attention layer\n",
    "# x = self_attention([x, x])\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs, name='train-head')\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=TRAIN_LR, global_clipnorm=3.0),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Add validation_split for proper monitoring\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=TRAIN_ES_PATIENCE,\n",
    "                                                           min_delta=ES_LR_MIN_DELTA, restore_best_weights=True)\n",
    "learning_rate_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=TRAIN_LR_PATIENCE,\n",
    "                                                              verbose=0, min_delta=ES_LR_MIN_DELTA, min_lr=TRAIN_MIN_LR)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=TRAIN_EPOCH, batch_size=BATCH_SIZE, verbose=1,\n",
    "                    class_weight=class_weights, validation_split=0.2,\n",
    "                    callbacks=[early_stopping_callback, learning_rate_callback])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Initial training - Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Model Finetuning\n",
    "print(\"\\nFinetuning ...\")\n",
    "unfreeze = 59\n",
    "base_model.trainable = True\n",
    "fine_tune_from = len(base_model.layers) - unfreeze\n",
    "for layer in base_model.layers[:fine_tune_from]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[fine_tune_from:]:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "# Rebuild model for finetuning\n",
    "inputs = input_layer\n",
    "x = sample_resizing(inputs)\n",
    "x = data_augmentation(x)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = patch_extraction(x)\n",
    "x = tf.keras.layers.SpatialDropout2D(FT_DROPOUT)(x)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(FT_DROPOUT)(x)\n",
    "x = pre_classification(x)\n",
    "# Remove the self_attention layer here too\n",
    "# x = self_attention([x, x])\n",
    "x = tf.keras.layers.Dropout(FT_DROPOUT)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs, name='finetune-backbone')\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=FT_LR, global_clipnorm=3.0),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training Procedure\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=ES_LR_MIN_DELTA,\n",
    "                                                           patience=FT_ES_PATIENCE, restore_best_weights=True)\n",
    "\n",
    "# Fix: Use ReduceLROnPlateau instead of InverseTimeDecay to avoid float conversion issues\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                                          factor=0.5,\n",
    "                                                          patience=5,\n",
    "                                                          min_lr=1e-7,\n",
    "                                                          verbose=1)\n",
    "\n",
    "history_finetune = model.fit(X_train, y_train, epochs=FT_EPOCH, batch_size=BATCH_SIZE, verbose=1,\n",
    "                             validation_split=0.2,\n",
    "                             initial_epoch=len(history.epoch) if history.epoch else 0,\n",
    "                             callbacks=[early_stopping_callback, reduce_lr_callback, tensorboard_callback])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Final test accuracy: {test_acc:.4f}\")\n",
    "model.save('model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
