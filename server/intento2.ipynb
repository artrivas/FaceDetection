{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127e8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"subhaditya/fer2013plus\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "import os\n",
    "import cv2\n",
    "import h5py\n",
    "import numpy as np\n",
    "import h5py\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "def load_data(\n",
    "    path_prefix,\n",
    "    dataset_name,\n",
    "    splits=['train', 'test'],\n",
    "):\n",
    "    X, y = {}, {}\n",
    "\n",
    "    IMG_SIZE = 224 if 'RAFDB' in dataset_name else 120\n",
    "    splits = ['train', 'test'] if 'RAFDB' in dataset_name else splits\n",
    "    classNames = ['anger', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise'] if 'RAFDB' in dataset_name else ['anger', 'contempt', 'disgust', 'fear', 'happiness', 'neutral', 'sadness', 'surprise']\n",
    "\n",
    "    for split in splits:\n",
    "        PATH = os.path.join(path_prefix, dataset_name, split)\n",
    "        X[split], y[split] = [], []\n",
    "        for classes in os.listdir(PATH):\n",
    "            class_path = os.path.join(PATH, classes)\n",
    "            class_numeric = classNames.index(classes)\n",
    "            for sample in os.listdir(class_path):\n",
    "                sample_path = os.path.join(class_path, sample)\n",
    "                image = cv2.imread(sample_path, cv2.IMREAD_COLOR)\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "                X[split].append(image)\n",
    "                y[split].append(class_numeric)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    for split in splits:\n",
    "        X[split] = np.array(X[split])\n",
    "        y[split] = np.array(y[split])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = load_data('../kaggle/input/fer2013plus/fer2013plus', dataset_name='fer2013')\n",
    "\n",
    "with h5py.File('ferp.h5', 'w') as dataset:\n",
    "    for split in X.keys():\n",
    "        dataset.create_dataset(f'X_{split}', data=X[split])\n",
    "        dataset.create_dataset(f'y_{split}', data=y[split])\n",
    "\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0257e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 8\n",
    "IMG_SHAPE = (120, 120, 3)\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "TRAIN_EPOCH = 100\n",
    "TRAIN_LR = 1e-3\n",
    "TRAIN_ES_PATIENCE = 5\n",
    "TRAIN_LR_PATIENCE = 3\n",
    "TRAIN_MIN_LR = 1e-6\n",
    "TRAIN_DROPOUT = 0.1\n",
    "\n",
    "FT_EPOCH = 500\n",
    "FT_LR = 1e-5\n",
    "FT_LR_DECAY_STEP = 80.0\n",
    "FT_LR_DECAY_RATE = 1\n",
    "FT_ES_PATIENCE = 20\n",
    "FT_DROPOUT = 0.2\n",
    "\n",
    "ES_LR_MIN_DELTA = 0.003\n",
    "\n",
    "# Load your data here, PAtt-Lite was trained with h5py for shorter loading time\n",
    "with h5py.File('ferp.h5', 'r') as dataset:\n",
    "    X_train = np.array(dataset['X_train'])\n",
    "    y_train = np.array(dataset['y_train'])\n",
    "    X_test = np.array(dataset['X_test'])\n",
    "    y_test = np.array(dataset['y_test'])\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "\n",
    "print(\"Shape of train_sample: {}\".format(X_train.shape))\n",
    "print(\"Shape of train_label: {}\".format(y_train.shape))\n",
    "print(\"Shape of test_sample: {}\".format(X_test.shape))\n",
    "print(\"Shape of test_label: {}\".format(y_test.shape))\n",
    "\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Model Building - PAtt-Lite Architecture\n",
    "input_layer = tf.keras.Input(shape=IMG_SHAPE, name='universal_input')\n",
    "sample_resizing = tf.keras.layers.Resizing(224, 224, name=\"resize\")\n",
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(mode='horizontal'),\n",
    "                                        tf.keras.layers.RandomContrast(factor=0.3)], name=\"augmentation\")\n",
    "preprocess_input = tf.keras.applications.mobilenet.preprocess_input\n",
    "\n",
    "backbone = tf.keras.applications.mobilenet.MobileNet(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "backbone.trainable = False\n",
    "base_model = tf.keras.Model(backbone.input, backbone.layers[-29].output, name='base_model')\n",
    "\n",
    "# Self-attention layer for PAtt-Lite\n",
    "self_attention = tf.keras.layers.Attention(use_scale=True, name='attention')\n",
    "\n",
    "patch_extraction = tf.keras.Sequential([\n",
    "    tf.keras.layers.SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'),\n",
    "    tf.keras.layers.SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'),\n",
    "    tf.keras.layers.Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n",
    "], name='patch_extraction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom layer for reshaping spatial features for attention\n",
    "class SpatialAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, use_scale=True, name=None):\n",
    "        super(SpatialAttentionLayer, self).__init__(name=name)\n",
    "        self.attention = tf.keras.layers.Attention(use_scale=use_scale)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # inputs shape: (batch, height, width, channels)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        height = tf.shape(inputs)[1]\n",
    "        width = tf.shape(inputs)[2]\n",
    "        channels = tf.shape(inputs)[3]\n",
    "\n",
    "        # Reshape to (batch, height*width, channels)\n",
    "        x_reshaped = tf.reshape(inputs, (batch_size, height * width, channels))\n",
    "\n",
    "        # Apply attention\n",
    "        x_attention = self.attention([x_reshaped, x_reshaped])\n",
    "\n",
    "        # Reshape back to (batch, height, width, channels)\n",
    "        x_output = tf.reshape(x_attention, (batch_size, height, width, channels))\n",
    "\n",
    "        return x_output\n",
    "\n",
    "# Modified approach: Apply attention to spatial features before pooling\n",
    "def create_patt_lite_model(input_layer, training_phase=True):\n",
    "    inputs = input_layer\n",
    "    x = sample_resizing(inputs)\n",
    "    x = data_augmentation(x)\n",
    "    x = preprocess_input(x)\n",
    "    x = base_model(x, training=False)\n",
    "    x = patch_extraction(x)\n",
    "\n",
    "    # Apply spatial attention using custom layer\n",
    "    spatial_attention = SpatialAttentionLayer(use_scale=True, name='spatial_attention')\n",
    "    x_attended = spatial_attention(x)\n",
    "\n",
    "    # Apply dropout and pooling based on training phase\n",
    "    if training_phase:\n",
    "        x_pooled = tf.keras.layers.GlobalAveragePooling2D(name='gap')(x_attended)\n",
    "        x_pooled = tf.keras.layers.Dropout(TRAIN_DROPOUT)(x_pooled)\n",
    "    else:\n",
    "        x_dropout = tf.keras.layers.SpatialDropout2D(FT_DROPOUT)(x_attended)\n",
    "        x_pooled = tf.keras.layers.GlobalAveragePooling2D(name='gap_ft')(x_dropout)\n",
    "        x_pooled = tf.keras.layers.Dropout(FT_DROPOUT)(x_pooled)\n",
    "\n",
    "    # Pre-classification layers\n",
    "    x_pre = tf.keras.layers.Dense(32, activation='relu', name='pre_dense')(x_pooled)\n",
    "    x_pre = tf.keras.layers.BatchNormalization(name='pre_bn')(x_pre)\n",
    "\n",
    "    if not training_phase:\n",
    "        x_pre = tf.keras.layers.Dropout(FT_DROPOUT)(x_pre)\n",
    "\n",
    "    # Final classification\n",
    "    outputs = tf.keras.layers.Dense(NUM_CLASSES, activation=\"softmax\", name='classification_head')(x_pre)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Create initial training model\n",
    "model = create_patt_lite_model(input_layer, training_phase=True)\n",
    "model._name = 'train-head'\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=TRAIN_LR, global_clipnorm=3.0),\n",
    "              loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(\"Model summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Training callbacks\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=TRAIN_ES_PATIENCE,\n",
    "                                                           min_delta=ES_LR_MIN_DELTA, restore_best_weights=True)\n",
    "learning_rate_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', patience=TRAIN_LR_PATIENCE,\n",
    "                                                              verbose=0, min_delta=ES_LR_MIN_DELTA, min_lr=TRAIN_MIN_LR)\n",
    "\n",
    "# Initial training\n",
    "print(\"Starting initial training...\")\n",
    "history = model.fit(X_train, y_train, epochs=TRAIN_EPOCH, batch_size=BATCH_SIZE, verbose=1,\n",
    "                    class_weight=class_weights, validation_split=0.2,\n",
    "                    callbacks=[early_stopping_callback, learning_rate_callback])\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Initial training - Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05648b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Finetuning\n",
    "print(\"\\nFinetuning ...\")\n",
    "unfreeze = 59\n",
    "base_model.trainable = True\n",
    "fine_tune_from = len(base_model.layers) - unfreeze\n",
    "for layer in base_model.layers[:fine_tune_from]:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[fine_tune_from:]:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "# Create finetuning model\n",
    "model_ft = create_patt_lite_model(input_layer, training_phase=False)\n",
    "model_ft._name = 'finetune-backbone'\n",
    "\n",
    "model_ft.compile(optimizer=keras.optimizers.Adam(learning_rate=FT_LR, global_clipnorm=3.0),\n",
    "                 loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Finetuning callbacks\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=ES_LR_MIN_DELTA,\n",
    "                                                           patience=FT_ES_PATIENCE, restore_best_weights=True)\n",
    "scheduler = keras.optimizers.schedules.InverseTimeDecay(initial_learning_rate=FT_LR,\n",
    "                                                       decay_steps=FT_LR_DECAY_STEP,\n",
    "                                                       decay_rate=FT_LR_DECAY_RATE)\n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(schedule=scheduler)\n",
    "\n",
    "# Fix: Use ReduceLROnPlateau instead of InverseTimeDecay to avoid float conversion issues\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',\n",
    "                                                          factor=0.5,\n",
    "                                                          patience=5,\n",
    "                                                          min_lr=1e-7,\n",
    "                                                          verbose=1)\n",
    "\n",
    "# Finetuning training\n",
    "print(\"Starting finetuning...\")\n",
    "history_finetune = model_ft.fit(X_train, y_train, epochs=FT_EPOCH, batch_size=BATCH_SIZE, verbose=1,\n",
    "                                validation_split=0.2,\n",
    "                                initial_epoch=len(history.epoch) if history.epoch else 0,\n",
    "                                callbacks=[early_stopping_callback, reduce_lr_callback, tensorboard_callback])\n",
    "\n",
    "test_loss, test_acc = model_ft.evaluate(X_test, y_test)\n",
    "print(f\"Final test accuracy: {test_acc:.4f}\")\n",
    "model_ft.save('model.h5')\n",
    "#supuestamente es mejor guardarlo de la siguiente manera\n",
    "model.save('model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff75014",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
